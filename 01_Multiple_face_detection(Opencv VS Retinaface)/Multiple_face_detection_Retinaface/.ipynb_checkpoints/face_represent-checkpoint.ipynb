{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d469c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace\n",
    "from deepface.commons import functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e6dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model input shape:  []\n",
      "model output shape:  2622\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "#build face recognition model\n",
    "\n",
    "model = VGGFace.loadModel()\n",
    "#model = Facenet.loadModel()\n",
    "#model = OpenFace.loadModel()\n",
    "#model = FbDeepFace.loadModel()\n",
    "\n",
    "try:\n",
    "\tinput_shape = model.layers[0].input_shape[1:3]\n",
    "except: #issue 470\n",
    "\tinput_shape = model.layers[0].input_shape[0][1:3]\n",
    "\n",
    "print(\"model input shape: \", model.layers[0].input_shape[1:])\n",
    "print(\"model output shape: \", model.layers[-1].input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f4f86a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#----------------------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#load images and find embeddings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#img1 = functions.detectFace(\"dataset/img1.jpg\", input_shape)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m img1 \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_face\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtests/dataset/img1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m img1_representation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(img1)[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#img2 = functions.detectFace(\"dataset/img3.jpg\", input_shape)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deepface\\commons\\functions.py:200\u001b[0m, in \u001b[0;36mpreprocess_face\u001b[1;34m(img, target_size, grayscale, enforce_detection, detector_backend, return_region, align)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m#---------------------------------------------------\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m#resize image to expected shape\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# img = cv2.resize(img, target_size) #resize causes transformation on base image, adding black pixels to resize will not deform the base image\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 200\u001b[0m \tfactor_0 \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    201\u001b[0m \tfactor_1 \u001b[38;5;241m=\u001b[39m target_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    202\u001b[0m \tfactor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(factor_0, factor_1)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------\n",
    "#load images and find embeddings\n",
    "\n",
    "#img1 = functions.detectFace(\"dataset/img1.jpg\", input_shape)\n",
    "img1 = functions.preprocess_face(\"tests/dataset/img1.jpg\", input_shape)\n",
    "img1_representation = model.predict(img1)[0,:]\n",
    "\n",
    "#img2 = functions.detectFace(\"dataset/img3.jpg\", input_shape)\n",
    "img2 = functions.preprocess_face(\"tests/dataset/img3.jpg\", input_shape)\n",
    "img2_representation = model.predict(img2)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab68bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
